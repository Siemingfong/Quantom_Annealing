{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2023 Ming-Fong Sie <seansie07@gmail.com> & Yu-Jing Lin <elvisyjlin@gmail.com>\n",
    "\n",
    "This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "4.0 International License. To view a copy of this license, visit\n",
    "http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "Creative Commons, PO Box 1866, Mountain View, CA 94042, USA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Abbreviations\n",
    "1. cm: confusion matrix\n",
    "2. rp: classification report\n",
    "3. fi: feature importance\n",
    "4. if: important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all used packages\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score  # Add this import statement at the beginning\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from learn import get_model, get_params\n",
    "from utils import run_from_ipython, np2df\n",
    "from viz import show_cm_list, show_rp_list\n",
    "\n",
    "if run_from_ipython():\n",
    "    import matplotlib\n",
    "    # %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import xgboost\n",
    "    sns.set_context('notebook')  # 'notebook', 'paper', 'talk', 'poster'\n",
    "    # sns.set_style('dark')  # None, 'darkgrid', 'whitegrid', 'dark', 'white', 'ticks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(feature_type='bem', model='svm', n_folds=10, n_jobs=10, no_cost_sentitive=False, output='./data_p', result='./result', scheme='address')\n"
     ]
    }
   ],
   "source": [
    "# Parse Arguments\n",
    "\n",
    "def parse(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog='Classification',\n",
    "        description='Train and test a machine learning classification method on the extracted features.'\n",
    "    )\n",
    "    parser.add_argument('--n_folds', help='n folds cross validation', type=int, default=10)\n",
    "    parser.add_argument('--model', '-m', help='model/method', type=str,\n",
    "                        choices=['lr', 'p', 'ab', 'rf', 'svm', 'xgb', 'lgb'], default='svm')\n",
    "    parser.add_argument('--feature_type', '-f',\n",
    "                        help='feature type (\"b\" | \"e\" | \"m\" or \"if4\", \"if5\", \"if10\", \"if13\", \"if20\", \"if64\")',\n",
    "                        type=str, default='bem')\n",
    "    parser.add_argument('--scheme', '-s', help='data scheme', type=str,\n",
    "                        choices=['address', 'entity'], default='address')\n",
    "    parser.add_argument('--n_jobs', '-j', help='number of workers/threads; set -1 to use all processors', type=int, default=10)\n",
    "    parser.add_argument('--no_cost_sentitive', help='disable cost sentitive learning', action='store_true')\n",
    "    parser.add_argument('--output', '-o', help='output path', type=str, default='./data_p')\n",
    "    parser.add_argument('--result', '-r', help='result path', type=str, default='./result')\n",
    "    return parser.parse_args() if args is None else parser.parse_args(args)\n",
    "args = parse([]) if run_from_ipython() else parse()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Setting\n",
      "===> Model:          svm\n",
      "===> Feature Types:  bem\n",
      "===> Data Scheme:    address\n",
      "===> Cost Sensitive: True\n",
      "===> N Threads:      10\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment setting\n",
    "\n",
    "n_folds = args.n_folds                       # 10\n",
    "model = args.model                           # 'lr', 'p', 'ab', 'rf', 'svm', 'xgb', 'lgb'\n",
    "feature_type = args.feature_type             # 'b' | 'e' | 'm' or 'if4', 'if5', 'if10', 'if13', 'if20', 'if64'\n",
    "scheme = args.scheme                         # 'address', 'entity'\n",
    "n_jobs = args.n_jobs                         # -1 to use all processors, or any positive integer\n",
    "cost_sensitive = not args.no_cost_sentitive  # True, False\n",
    "output_path = args.output\n",
    "result_path = args.result\n",
    "\n",
    "# Check the experiment setting\n",
    "\n",
    "assert model in ['lr', 'p', 'ab', 'rf', 'svm', 'xgb', 'lgb']\n",
    "assert not feature_type.startswith('if') and len(feature_type) > 0 or \\\n",
    "       feature_type.startswith('if') and feature_type[2:].isdigit()\n",
    "# assert scheme in ['address', 'entity']\n",
    "assert scheme in ['address']\n",
    "\n",
    "# Show the experiment setting\n",
    "\n",
    "print('Experiment Setting')\n",
    "print('===> Model:         ', model)\n",
    "print('===> Feature Types: ', feature_type)\n",
    "print('===> Data Scheme:   ', scheme)\n",
    "print('===> Cost Sensitive:', cost_sensitive)\n",
    "print('===> N Threads:     ', n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義一個函數來加載和抽樣數據\n",
    "def load_sample(file_path, sample_fraction=0.1):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=100000):\n",
    "        chunks.append(chunk.sample(frac=sample_fraction))\n",
    "    return pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             n_tx  total_days  total_spent_btc  total_received_btc  \\\n",
      "50014   -0.021552   -0.400931        -0.017221           -0.017221   \n",
      "26354    0.773930   -0.316640        -0.019504           -0.019504   \n",
      "95154   -0.180649   -0.400931        -0.021285           -0.021285   \n",
      "98537   -0.101101   -0.391565        -0.018448           -0.018448   \n",
      "14351   -0.101101   -0.400931        -0.017662           -0.017662   \n",
      "...           ...         ...              ...                 ...   \n",
      "1809284 -0.180649   -0.400931        -0.020233           -0.020233   \n",
      "1820681 -0.180649   -0.400931        -0.021377           -0.021377   \n",
      "1812420 -0.101101   -0.400931        -0.019546           -0.019546   \n",
      "1807144  0.057996    0.067352        -0.020652           -0.020652   \n",
      "1852275 -0.180649   -0.400931        -0.018820           -0.018820   \n",
      "\n",
      "         total_spent_usd  total_received_usd  mean_balance_btc  \\\n",
      "50014          -0.007897           -0.007897         -0.085138   \n",
      "26354          -0.013890           -0.013890         -0.128108   \n",
      "95154          -0.014118           -0.014118         -0.129463   \n",
      "98537          -0.013746           -0.013746         -0.082336   \n",
      "14351          -0.013934           -0.013934         -0.068761   \n",
      "...                  ...                 ...               ...   \n",
      "1809284        -0.013698           -0.013698         -0.093158   \n",
      "1820681        -0.014164           -0.014164         -0.132648   \n",
      "1812420        -0.009189           -0.009189         -0.101286   \n",
      "1807144        -0.013782           -0.013782         -0.126740   \n",
      "1852275        -0.014011           -0.014011         -0.044374   \n",
      "\n",
      "         std_balance_btc  mean_balance_usd  std_balance_usd  ...  \\\n",
      "50014          -0.002451         -0.006527        -0.002642  ...   \n",
      "26354          -0.002451         -0.061980        -0.002643  ...   \n",
      "95154          -0.002451         -0.060662        -0.002643  ...   \n",
      "98537          -0.002451         -0.056648        -0.002643  ...   \n",
      "14351          -0.002450         -0.059159        -0.002643  ...   \n",
      "...                  ...               ...              ...  ...   \n",
      "1809284        -0.002451         -0.049412        -0.002643  ...   \n",
      "1820681        -0.002451         -0.061887        -0.002643  ...   \n",
      "1812420        -0.002451          0.004241        -0.002640  ...   \n",
      "1807144        -0.002451         -0.059865        -0.002643  ...   \n",
      "1852275        -0.002451         -0.057798        -0.002643  ...   \n",
      "\n",
      "         dist_payback_1st_moment  dist_payback_2nd_moment  \\\n",
      "50014                  -0.385631                -0.173044   \n",
      "26354                  -0.271725                -0.172000   \n",
      "95154                  -0.389038                -0.173049   \n",
      "98537                  -0.366401                -0.172924   \n",
      "14351                  -0.389038                -0.173049   \n",
      "...                          ...                      ...   \n",
      "1809284                -0.389038                -0.173049   \n",
      "1820681                -0.389038                -0.173049   \n",
      "1812420                -0.381341                -0.173034   \n",
      "1807144                 0.149878                -0.134963   \n",
      "1852275                -0.389038                -0.173049   \n",
      "\n",
      "         dist_payback_3rd_moment  dist_payback_4th_moment  tx_input  \\\n",
      "50014                   1.386656                 0.468250 -0.151659   \n",
      "26354                  -0.768797                 0.657007 -0.151659   \n",
      "95154                  -0.132643                -0.518267 -0.039017   \n",
      "98537                  -0.132643                 0.139411  0.073625   \n",
      "14351                  -0.132643                -0.518267 -0.151659   \n",
      "...                          ...                      ...       ...   \n",
      "1809284                -0.132643                -0.518267 -0.151659   \n",
      "1820681                -0.132643                -0.518267 -0.151659   \n",
      "1812420                -0.132643                 0.139411 -0.151659   \n",
      "1807144                -0.569448                 0.376572 -0.151659   \n",
      "1852275                -0.132643                -0.518267 -0.151659   \n",
      "\n",
      "         tx_output  n_multi_in  n_multi_out  n_multi_in_out  class  \n",
      "50014    -0.055827   -0.219166    -0.050256       -0.657671      0  \n",
      "26354    -0.055827   -0.018729    -0.050256        1.520518      0  \n",
      "95154     0.026471   -0.018729     0.028092        1.520518      0  \n",
      "98537    -0.055827    0.081489    -0.050256        1.520518      2  \n",
      "14351    -0.055827   -0.219166    -0.050256       -0.657671      4  \n",
      "...            ...         ...          ...             ...    ...  \n",
      "1809284  -0.055827   -0.219166    -0.050256       -0.657671      0  \n",
      "1820681  -0.055827   -0.219166    -0.050256       -0.657671      0  \n",
      "1812420  -0.138124   -0.219166    -0.206951       -0.657671      0  \n",
      "1807144  -0.055827   -0.219166    -0.050256       -0.657671      0  \n",
      "1852275  -0.138124   -0.219166    -0.206951       -0.657671      4  \n",
      "\n",
      "[185703 rows x 74 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load transaction history summarization data\n",
    "\n",
    "# 設定樣本比例\n",
    "sample_fraction = 0.1  # 取 10% 的數據樣本\n",
    "\n",
    "# data_file = 'data.{}.csv'.format(scheme)\n",
    "data_file = 'nanzero_normalization_data.{}.csv'.format(scheme)\n",
    "# data_file = 'quantum_qubo_data.{}.csv'.format(scheme)\n",
    "# data_file = 'all_selected_features_quantum_qubo_data.{}.csv'.format(scheme)\n",
    "file_path = os.path.join(output_path, data_file)\n",
    "data = load_sample(file_path, sample_fraction)\n",
    "# data = pd.read_csv(os.path.join(output_path, data_file))\n",
    "print (data)\n",
    "if run_from_ipython():\n",
    "    data.head(4)\n",
    "else:\n",
    "    print(data.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f_tx', 'f_received', 'f_coinbase', 'f_spent_digits_-3', 'f_spent_digits_-2', 'f_spent_digits_-1', 'f_spent_digits_0', 'f_spent_digits_1', 'f_spent_digits_2', 'f_spent_digits_3', 'f_spent_digits_4', 'f_spent_digits_5', 'f_spent_digits_6', 'f_received_digits_-3', 'f_received_digits_-2', 'f_received_digits_-1', 'f_received_digits_0', 'f_received_digits_1', 'f_received_digits_2', 'f_received_digits_3', 'f_received_digits_4', 'f_received_digits_5', 'f_received_digits_6', 'r_payback', 'n_inputs_in_spent', 'n_outputs_in_spent', 'n_tx', 'total_days', 'n_spent', 'n_received', 'n_coinbase', 'n_payback', 'total_spent_btc', 'total_received_btc', 'total_spent_usd', 'total_received_usd', 'mean_balance_btc', 'std_balance_btc', 'mean_balance_usd', 'std_balance_usd', 'interval_1st_moment', 'interval_2nd_moment', 'interval_3rd_moment', 'interval_4th_moment', 'dist_total_1st_moment', 'dist_total_2nd_moment', 'dist_total_3rd_moment', 'dist_total_4th_moment', 'dist_coinbase_1st_moment', 'dist_coinbase_2nd_moment', 'dist_coinbase_3rd_moment', 'dist_coinbase_4th_moment', 'dist_spend_1st_moment', 'dist_spend_2nd_moment', 'dist_spend_3rd_moment', 'dist_spend_4th_moment', 'dist_receive_1st_moment', 'dist_receive_2nd_moment', 'dist_receive_3rd_moment', 'dist_receive_4th_moment', 'dist_payback_1st_moment', 'dist_payback_2nd_moment', 'dist_payback_3rd_moment', 'dist_payback_4th_moment']\n",
      "bem\n",
      "[[ 1.35592384  0.0030658  -0.0030658  ... -0.17304366  1.38665583\n",
      "   0.46825035]\n",
      " [ 0.29913865  0.0030658  -0.0030658  ... -0.17200047 -0.76879727\n",
      "   0.65700703]\n",
      " [ 0.11264715  0.0030658  -0.0030658  ... -0.17304888 -0.13264268\n",
      "  -0.51826731]\n",
      " ...\n",
      " [ 0.7342855   0.0030658  -0.0030658  ... -0.17303448 -0.13264268\n",
      "   0.13941113]\n",
      " [-0.46023525  0.0030658  -0.0030658  ... -0.13496293 -0.56944829\n",
      "   0.37657228]\n",
      " [ 0.11264715  0.0030658  -0.0030658  ... -0.17304888 -0.13264268\n",
      "  -0.51826731]]\n",
      "[0 0 0 ... 0 0 4]\n",
      "['Exchange' 'Faucet' 'Gambling' 'Market' 'Mixer' 'Pool']\n",
      "185703 185703 64\n"
     ]
    }
   ],
   "source": [
    "# Define 4 types of features (basic statistics, extra statistics, moments and patterns)\n",
    "\n",
    "basic = [\n",
    "    'f_tx', 'f_received', 'f_coinbase',\n",
    "    'f_spent_digits_-3', 'f_spent_digits_-2', 'f_spent_digits_-1', 'f_spent_digits_0',\n",
    "    'f_spent_digits_1', 'f_spent_digits_2', 'f_spent_digits_3', 'f_spent_digits_4',\n",
    "    'f_spent_digits_5', 'f_spent_digits_6', 'f_received_digits_-3', 'f_received_digits_-2',\n",
    "    'f_received_digits_-1', 'f_received_digits_0', 'f_received_digits_1', 'f_received_digits_2',\n",
    "    'f_received_digits_3', 'f_received_digits_4', 'f_received_digits_5', 'f_received_digits_6',\n",
    "    'r_payback', 'n_inputs_in_spent', 'n_outputs_in_spent'\n",
    "]\n",
    "extra = [\n",
    "    'n_tx', 'total_days', 'n_spent', 'n_received', 'n_coinbase', 'n_payback',\n",
    "    'total_spent_btc', 'total_received_btc',\n",
    "    'total_spent_usd', 'total_received_usd',\n",
    "    'mean_balance_btc', 'std_balance_btc',\n",
    "    'mean_balance_usd', 'std_balance_usd'\n",
    "]\n",
    "moments = [\n",
    "    'interval_1st_moment', 'interval_2nd_moment', 'interval_3rd_moment', 'interval_4th_moment',\n",
    "    'dist_total_1st_moment', 'dist_total_2nd_moment', 'dist_total_3rd_moment', 'dist_total_4th_moment',\n",
    "    'dist_coinbase_1st_moment', 'dist_coinbase_2nd_moment', 'dist_coinbase_3rd_moment', 'dist_coinbase_4th_moment',\n",
    "    'dist_spend_1st_moment', 'dist_spend_2nd_moment', 'dist_spend_3rd_moment', 'dist_spend_4th_moment',\n",
    "    'dist_receive_1st_moment', 'dist_receive_2nd_moment', 'dist_receive_3rd_moment', 'dist_receive_4th_moment',\n",
    "    'dist_payback_1st_moment', 'dist_payback_2nd_moment', 'dist_payback_3rd_moment', 'dist_payback_4th_moment'\n",
    "]\n",
    "patterns =[\n",
    "    'tx_input', 'tx_output',\n",
    "    'n_multi_in', 'n_multi_out', 'n_multi_in_out'\n",
    "]\n",
    "\n",
    "features = []\n",
    "if not feature_type.startswith('if') and len(feature_type) > 0:\n",
    "    if 'b' in feature_type:\n",
    "        features += basic\n",
    "    if 'e' in feature_type:\n",
    "        features += extra\n",
    "    if 'm' in feature_type:\n",
    "        features += moments\n",
    "    if 'p' in feature_type:\n",
    "        features += patterns\n",
    "        print(\"Patterns included:\", patterns)\n",
    "elif feature_type.startswith('if') and feature_type[2:].isdigit():\n",
    "    \"\"\"\n",
    "    Important features from LightGBM with BEM\n",
    "    [ 0 25 24 29 40 37 27 23 56 36  1 28 26 57 32 38 44 45 33 18 39 60 53 35\n",
    "     34 52 41 17 14 15 16 19 42  5  6 47  7 46  2 54  4 43  8 59 58 55  9 13\n",
    "     61 48  3 31 10 62 20 21 63 30 49 11 51 50 22 12]\n",
    "    \"\"\"\n",
    "    all_features = basic + extra + moments + patterns\n",
    "    if_indices = [\n",
    "        0, 25, 24, 29, 40, 37, 27, 23, 56, 36,\n",
    "        1, 28, 26, 57, 32, 38, 44, 45, 33, 18,\n",
    "        39, 60, 53, 35, 34, 52, 41, 17, 14, 15,\n",
    "        16, 19, 42, 5, 6, 47, 7, 46, 2, 54,\n",
    "        4, 43, 8, 59, 58, 55, 9, 13, 61, 48,\n",
    "        3, 31, 10, 62, 20, 21, 63, 30, 49, 11,\n",
    "        51, 50, 22, 12\n",
    "    ]\n",
    "    if_features = [all_features[i] for i in if_indices]\n",
    "    n_if = int(feature_type[2:])\n",
    "    features = if_features[:n_if]\n",
    "else:\n",
    "    raise Exception('Invalid feature types: {:s}'.format(feature_type))\n",
    "\n",
    "invalid_features = [feature for feature in features if feature not in data.columns]\n",
    "assert len(invalid_features) == 0, 'Invalid features: ' + ', '.join(invalid_features)\n",
    "\n",
    "X = data.get(features).values\n",
    "y = data['class'].values\n",
    "print (features)\n",
    "print (feature_type)\n",
    "print (X)\n",
    "print (y)\n",
    "\n",
    "class2label = json.loads(open(os.path.join(output_path, 'class2label.json'), 'r').read())\n",
    "label2class = json.loads(open(os.path.join(output_path, 'label2class.json'), 'r').read())\n",
    "class_names = np.array([label2class[i] for i in range(6)])\n",
    "print (class_names)\n",
    "y_names = class_names\n",
    "# y_names = class_names[y]\n",
    "# y_names = np.array(class_names)[y.astype(int)]\n",
    "\n",
    "print(len(X), len(y), len(features))\n",
    "\n",
    "os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_distribution(df):\n",
    "#     if run_from_ipython():\n",
    "#         plt.figure()\n",
    "#         sns.countplot(df.index)\n",
    "#     cnt = collections.Counter(df.index)\n",
    "#     print(cnt)\n",
    "#     return np.array([cnt[i] for i in range(len(cnt))])\n",
    "\n",
    "# print(class2label)\n",
    "# data_dict = np2df(X, y)\n",
    "# print(data_dict)\n",
    "# y_count = data_distribution(data_dict)\n",
    "# # y_count = data_distribution(np2df(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters:\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_cm_list = []\n",
    "train_rp_list = []\n",
    "valid_cm_list = []\n",
    "valid_rp_list = []\n",
    "fi_list = []\n",
    "\n",
    "# Add these lists to store AUC scores\n",
    "train_auc_list = []\n",
    "valid_auc_list = []\n",
    "\n",
    "# Add these lists to store training and validating time\n",
    "train_time_list = []\n",
    "valid_time_list = []\n",
    "\n",
    "# Model parameters\n",
    "clf_params = get_params(model)\n",
    "if model not in ['ab', 'svm']:\n",
    "    clf_params['n_jobs'] = n_jobs\n",
    "print('Hyper-parameters:')\n",
    "print(clf_params)\n",
    "\n",
    "# Declare K-Fold\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# Normalize data\n",
    "# Note that decision tree sbased algorithms need no data normalization\n",
    "# if model in ['lr', 'p', 'svm']:\n",
    "    \n",
    "#     # 初始化MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "    \n",
    "#     print('Normalizing data...')\n",
    "#     # X = np.nan_to_num(X / np.abs(X).max(axis=0))\n",
    "#     # 擬合數據並轉換\n",
    "#     X = scaler.fit_transform(X)\n",
    "\n",
    "# Start cross validation\n",
    "for train_idx, valid_idx in tqdm(skf.split(X, y)):\n",
    "    # print(train_idx[:100], valid_idx[:10])\n",
    "    \n",
    "    # Retrieve splitted training set and validating set\n",
    "    X_train, X_valid = X[train_idx], X[valid_idx]\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "    \n",
    "    # Calculate sample weight (whether to apply cost sensitive learning)\n",
    "    sample_weight = np.ones((len(y_train), ), dtype='float64')\n",
    "    if cost_sensitive:\n",
    "        sample_weight = class_weight.compute_sample_weight('balanced', y_train)\n",
    "    \n",
    "    # Declare the classifier and train it on the training set\n",
    "    clf = get_model(model, clf_params)\n",
    "\n",
    "    # Train the classifier and record the time\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    train_time = time.time() - start_time\n",
    "    train_time_list.append(train_time)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_pred = clf.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    cm = cm / cm.sum(axis=1, keepdims=True)\n",
    "    train_cm_list.append(cm)\n",
    "    rp = classification_report(y_train, y_pred, target_names=class_names, output_dict=True)\n",
    "    train_rp_list.append(rp)\n",
    "    \n",
    "    # Evaluate on the validating set and record the time\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    valid_time = time.time() - start_time\n",
    "    valid_time_list.append(valid_time)\n",
    "    \n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    cm = cm / cm.sum(axis=1, keepdims=True)\n",
    "    valid_cm_list.append(cm)\n",
    "    rp = classification_report(y_valid, y_pred, target_names=class_names, output_dict=True)\n",
    "    valid_rp_list.append(rp)\n",
    "    \n",
    "    # Get the feature importances according to the trained model\n",
    "    if model in ['rf', 'xgb', 'lgb']:\n",
    "        if 'booster' in clf_params and clf_params['booster'] == 'dart':\n",
    "            pass\n",
    "        else:\n",
    "            fi = clf.feature_importances_\n",
    "            fi_list.append(fi)\n",
    "            \n",
    "    # # Calculate AUC for the training set\n",
    "    # y_train_prob = clf.predict_proba(X_train)\n",
    "    # train_auc = roc_auc_score(y_train, y_train_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "    # train_auc_list.append(train_auc)\n",
    "\n",
    "    # # Calculate AUC for the validation set\n",
    "    # y_valid_prob = clf.predict_proba(X_valid)\n",
    "    # valid_auc = roc_auc_score(y_valid, y_valid_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "    # valid_auc_list.append(valid_auc)\n",
    "    \n",
    "# Print the average training and validation times\n",
    "print(f'Average training time: {sum(train_time_list) / len(train_time_list):.2f} seconds')\n",
    "print(f'Average validation time: {sum(valid_time_list) / len(valid_time_list):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "\n",
    "experiment_name = os.path.join(result_path, '{}.{}.{}'.format(model, feature_type, scheme))\n",
    "if not cost_sensitive:\n",
    "    experiment_name += '.no_cs'\n",
    "results = {\n",
    "    'train_cm_list': train_cm_list,\n",
    "    'valid_cm_list': valid_cm_list,\n",
    "    'train_rp_list': train_rp_list,\n",
    "    'valid_rp_list': valid_rp_list,\n",
    "    'fi_list': fi_list\n",
    "}\n",
    "pickle.dump(results, open(experiment_name + '.pkl', 'wb'))\n",
    "\n",
    "# Save model\n",
    "model_save_path = os.path.join(result_path, '{}_model.pkl'.format(experiment_name))\n",
    "with open(model_save_path, 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "print(f\"Results and model saved to {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average confusion matrix of training set in K-fold\n",
    "\n",
    "print('Average confusion matrix of training set in {:d}-fold'.format(n_folds))\n",
    "show_cm_list(train_cm_list, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    summed_metrics = {}\n",
    "    summed_accuracy = 0\n",
    "    summed_category_accuracies = {category: 0 for category in class_names}\n",
    "    category_supports = {category: 0 for category in class_names}\n",
    "\n",
    "    for report_index, report in enumerate(rp_list):\n",
    "        for category, metrics in report.items():\n",
    "            if category == 'accuracy':\n",
    "                summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            if isinstance(metrics, dict) and category in class_names:\n",
    "                if category not in summed_metrics:\n",
    "                    summed_metrics[category] = {key: 0 for key in metrics if key != 'support'}\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric != 'support':\n",
    "                        summed_metrics[category][metric] += value\n",
    "                category_supports[category] += metrics.get('support', 0)\n",
    "\n",
    "        cm = cm_list[report_index]\n",
    "        for i, category in enumerate(class_names):\n",
    "            TP = cm[i, i]\n",
    "            TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "            summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    avg_metrics = {}\n",
    "    total_support = sum(category_supports.values())\n",
    "    macro_avg = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "    weighted_avg = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "\n",
    "    for category, metrics in summed_metrics.items():\n",
    "        avg_metrics[category] = {metric: value / len(rp_list) for metric, value in metrics.items()}\n",
    "        for metric in macro_avg:\n",
    "            macro_avg[metric] += avg_metrics[category][metric] / len(class_names)\n",
    "            weighted_avg[metric] += (avg_metrics[category][metric] * category_supports[category]) / total_support\n",
    "\n",
    "    avg_accuracy = summed_accuracy / len(rp_list)\n",
    "    avg_category_accuracies = {category: acc / len(cm_list) for category, acc in summed_category_accuracies.items()}\n",
    "\n",
    "    print(f\"Overall Accuracy: {avg_accuracy}\\n\")\n",
    "    for category, metrics in avg_metrics.items():\n",
    "        print(f\"Category: {category}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "        print(f\"  Accuracy: {avg_category_accuracies[category]}\\n\")\n",
    "\n",
    "    print(\"Category: macro avg\")\n",
    "    for metric, value in macro_avg.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\nCategory: weighted avg\")\n",
    "    for metric, value in weighted_avg.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Assuming n_folds, train_rp_list, train_cm_list, and class_names are defined elsewhere\n",
    "print('Average classification report and accuracies of training set in {:d}-fold'.format(n_folds))\n",
    "show_rp_list_and_accuracies(train_rp_list, train_cm_list, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average confusion matrix of validataion set in K-fold\n",
    "\n",
    "print('Average confusion matrix of validataion set in {:d}-fold'.format(n_folds))\n",
    "show_cm_list(valid_cm_list, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    class_names_list = list(class_names)  # Convert class_names to a list for index access\n",
    "    summed_metrics, avg_metrics = {}, {}\n",
    "    summed_category_accuracies = {category: 0 for category in class_names}\n",
    "    summed_accuracy = 0\n",
    "\n",
    "    for report in rp_list:\n",
    "        for category, metrics in report.items():\n",
    "            if category == 'accuracy':\n",
    "                summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            if category in class_names_list and isinstance(metrics, dict):\n",
    "                if category not in summed_metrics:\n",
    "                    summed_metrics[category] = {key: 0 for key in metrics if key != 'support'}\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric != 'support':\n",
    "                        summed_metrics[category][metric] += value\n",
    "\n",
    "    for cm in cm_list:\n",
    "        for i, category in enumerate(class_names_list):\n",
    "            if category in summed_metrics:  # Ensure category exists in the metrics dictionary\n",
    "                TP = cm[i, i]\n",
    "                TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "                FP = cm[:, i].sum() - TP\n",
    "                FN = cm[i, :].sum() - TP\n",
    "                accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "                summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    # Calculate average metrics and accuracies\n",
    "    for category in class_names_list:\n",
    "        if category in summed_metrics:\n",
    "            avg_metrics[category] = {metric: summed_metrics[category][metric] / len(rp_list) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    \n",
    "    # Calculate macro and weighted averages\n",
    "    macro_avg = {metric: sum(avg_metrics[cat][metric] for cat in class_names_list) / len(class_names_list) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    weighted_avg = {metric: sum(avg_metrics[cat][metric] * summed_category_accuracies[cat] for cat in class_names_list) / sum(summed_category_accuracies.values()) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    \n",
    "    # Append macro and weighted averages to avg_metrics\n",
    "    avg_metrics['macro avg'] = macro_avg\n",
    "    avg_metrics['weighted avg'] = weighted_avg\n",
    "\n",
    "    avg_accuracy = summed_accuracy / len(rp_list) if len(rp_list) > 0 else 0\n",
    "    avg_category_accuracies = {category: summed_category_accuracies[category] / len(cm_list) for category in class_names_list}\n",
    "\n",
    "    return avg_metrics, avg_category_accuracies, avg_accuracy\n",
    "\n",
    "# Assuming necessary variables (n_folds, valid_rp_list, valid_cm_list, class_names) are defined\n",
    "print('Average classification report of validation set in {:d}-fold'.format(n_folds))\n",
    "avg_metrics, avg_category_accuracies, avg_accuracy = show_rp_list_and_accuracies(valid_rp_list, valid_cm_list, class_names)\n",
    "\n",
    "df_averages = pd.DataFrame(avg_metrics).T\n",
    "df_averages['accuracy'] = pd.Series(avg_category_accuracies)\n",
    "\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics for Validation Set')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    # Initialize dictionaries for summed and average metrics\n",
    "    summed_metrics, avg_metrics = {}, {}\n",
    "    summed_category_accuracies, avg_category_accuracies = {}, {}\n",
    "    summed_accuracy = 0\n",
    "\n",
    "    # Initialize summed metrics and accuracies\n",
    "    for category in class_names:\n",
    "        summed_metrics[category] = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "        summed_category_accuracies[category] = 0\n",
    "\n",
    "    # Iterate over reports and confusion matrices\n",
    "    for report_index, report in enumerate(rp_list):\n",
    "        for category, metrics in report.items():\n",
    "            # Skip 'accuracy', 'macro avg', and 'weighted avg' categories\n",
    "            if category in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                if category == 'accuracy':\n",
    "                    summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            # Sum metrics for each category\n",
    "            for metric in ['precision', 'recall', 'f1-score']:\n",
    "                if metric in metrics:\n",
    "                    summed_metrics[category][metric] += metrics[metric]\n",
    "\n",
    "        # Calculate and sum category-specific accuracies\n",
    "        cm = cm_list[report_index]\n",
    "        for i, category in enumerate(class_names):\n",
    "            TP = cm[i, i]\n",
    "            TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "            summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    # Calculate average metrics and accuracies\n",
    "    for category in class_names:\n",
    "        avg_metrics[category] = {metric: value / len(rp_list) for metric, value in summed_metrics[category].items()}\n",
    "        avg_category_accuracies[category] = summed_category_accuracies[category] / len(cm_list)\n",
    "    avg_accuracy = summed_accuracy / len(rp_list)\n",
    "\n",
    "    # Return average metrics and accuracies\n",
    "    return avg_metrics, avg_category_accuracies, avg_accuracy\n",
    "\n",
    "# Use the function for validation set and get the data for plotting\n",
    "avg_metrics, avg_category_accuracies, avg_accuracy = show_rp_list_and_accuracies(valid_rp_list, valid_cm_list, class_names)\n",
    "\n",
    "# Prepare DataFrame for plotting\n",
    "df_averages = pd.DataFrame(avg_metrics).T\n",
    "df_averages['accuracy'] = pd.Series(avg_category_accuracies)\n",
    "\n",
    "# Plotting\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics for Validation Set')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add value labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "if len(fi_list) == 0:\n",
    "    exit()\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    print(clf.importance_type)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "fi_avg = np.mean(fi_list, axis=0)\n",
    "# print(fi_avg)\n",
    "print(fi_avg.argsort()[::-1])\n",
    "if run_from_ipython():\n",
    "    df_feature_importances = pd.DataFrame({'name': features, 'importance': fi_avg})\n",
    "    df_top_10 = df_feature_importances.nlargest(10, columns='importance')\n",
    "    plt.figure()\n",
    "    sns.barplot(x='importance', y='name', data=df_top_10)  # Modified this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_from_ipython():\n",
    "    plt.figure(figsize=(16, 32))\n",
    "    for i in range(10):\n",
    "        feature_index = df_top_10.index[i]\n",
    "        feature_name = list(df_top_10['name'])[i]\n",
    "        feature_data = X[:, feature_index]\n",
    "\n",
    "        # Check if the lengths match\n",
    "        if len(y_names) != len(feature_data):\n",
    "            print(f\"Length mismatch for feature '{feature_name}': Length of y_names is {len(y_names)}, length of feature data is {len(feature_data)}\")\n",
    "            continue  # Skip this iteration\n",
    "\n",
    "        plt.subplot(5, 2, i+1)\n",
    "        plt.title(feature_name)\n",
    "        ax = sns.boxplot(x=y_names, y=feature_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_avg = np.mean(fi_list, axis=0)\n",
    "# print(fi_avg)\n",
    "print(fi_avg.argsort()[::-1])\n",
    "if run_from_ipython():\n",
    "    df_feature_importances = pd.DataFrame({'name': features, 'importance': fi_avg})\n",
    "    df_least_10 = df_feature_importances.nsmallest(10, columns='importance')\n",
    "    plt.figure()\n",
    "    sns.barplot(x='importance', y='name', data=df_least_10)  # Corrected this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_avg = np.mean(fi_list, axis=0)\n",
    "print(fi_avg.argsort()[::-1])\n",
    "if run_from_ipython():\n",
    "    df_feature_importances = pd.DataFrame({'Feature Name': features, 'Importance': fi_avg})\n",
    "    df_least_10 = df_feature_importances.nlargest(len(features), columns='Importance')\n",
    "    plt.figure(figsize=(24, 48))\n",
    "    sns.set(font_scale=2)\n",
    "    sns.barplot(x='Importance', y='Feature Name', data=df_least_10)  # Corrected this line\n",
    "    sns.set(font_scale=1)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('top_10_and_last_10_feature_importance.png')  # Uncomment this line to save the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cm_avg = np.mean(valid_cm_list, axis=0)\n",
    "# print(cm_avg)\n",
    "if run_from_ipython():\n",
    "    df_cm = pd.DataFrame(cm_avg, index=class_names, columns=class_names)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    cmap = sns.light_palette('black', as_cmap=True)\n",
    "    # cmap = ListedColormap(['white'])\n",
    "    sns.set(font_scale=1.6)\n",
    "    sns.heatmap(df_cm.round(2), annot=True, square=True, cbar=False, cmap=cmap)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    sns.set(font_scale=1)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('confusion_matrix.png')  # Uncomment this line to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算平均混淆矩阵\n",
    "avg_train_cm = np.mean(train_cm_list, axis=0)\n",
    "avg_valid_cm = np.mean(valid_cm_list, axis=0)\n",
    "\n",
    "# 计算平均AUC分数\n",
    "avg_train_auc = np.mean(train_auc_list)\n",
    "avg_valid_auc = np.mean(valid_auc_list)\n",
    "\n",
    "# 展示平均混淆矩阵\n",
    "print(\"Average Training Confusion Matrix:\")\n",
    "print(avg_train_cm)\n",
    "print(\"\\nAverage Validation Confusion Matrix:\")\n",
    "print(avg_valid_cm)\n",
    "\n",
    "# 展示平均AUC分数\n",
    "print(f\"\\nAverage Train AUC: {avg_train_auc}\")\n",
    "print(f\"Average Valid AUC: {avg_valid_auc}\")\n",
    "\n",
    "# 绘制AUC分数图表\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_auc_list, label='Train AUC', marker='o')\n",
    "plt.plot([avg_train_auc] * len(train_auc_list), 'r--', label='Average Train AUC')\n",
    "plt.plot(valid_auc_list, label='Valid AUC', marker='o')\n",
    "plt.plot([avg_valid_auc] * len(valid_auc_list), 'g--', label='Average Valid AUC')\n",
    "plt.title('AUC Scores per Fold')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# 计算每个类别的FPR和TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = len(class_names)  # 类别的数量\n",
    "\n",
    "# 计算每个类别的ROC曲线和AUC分数\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_valid, y_valid_prob[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# 计算宏观平均ROC曲线和AUC分数\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# 绘制所有类别的ROC曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "# 绘制宏观平均ROC曲线\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "# 绘制对角线\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC and AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
