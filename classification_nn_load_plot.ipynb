{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2023 Ming-Fong Sie <seansie07@gmail.com> & Yu-Jing Lin <elvisyjlin@gmail.com>\n",
    "\n",
    "This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "4.0 International License. To view a copy of this license, visit\n",
    "http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "Creative Commons, PO Box 1866, Mountain View, CA 94042, USA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Abbreviations\n",
    "1. cm: confusion matrix\n",
    "2. rp: classification report\n",
    "3. fi: feature importance\n",
    "4. if: important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 定義讀取結果的函數\n",
    "def load_results(result_path, model_name, feature_type, scheme, cost_sensitive=False):\n",
    "    experiment_name = os.path.join(result_path, '{}.{}.{}'.format(model_name, feature_type, scheme))\n",
    "    # if not cost_sensitive:\n",
    "    #     experiment_name += '.no_cs'\n",
    "    results_path = experiment_name + '.pkl'\n",
    "    \n",
    "    with open(results_path, 'rb') as file:\n",
    "        results = pickle.load(file)\n",
    "    \n",
    "    return results, experiment_name\n",
    "\n",
    "# 定義顯示混淆矩陣列表的函數\n",
    "def show_cm_list(cm_list, class_names):\n",
    "    if len(cm_list) == 0:\n",
    "        raise ValueError(\"cm_list is empty\")\n",
    "    \n",
    "    # 確保每個混淆矩陣都是 2D 的\n",
    "    for cm in cm_list:\n",
    "        if cm.ndim != 2:\n",
    "            raise ValueError(f\"Confusion matrix in cm_list is not 2D. Shape={cm.shape}\")\n",
    "\n",
    "    avg_cm = np.mean(cm_list, axis=0)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.2f', cmap='viridis', xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Average Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# 定義顯示分類報告和準確率的函數\n",
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    class_names_list = list(class_names)  # Convert class_names to a list for index access\n",
    "    summed_metrics, avg_metrics = {}, {}\n",
    "    summed_category_accuracies = {category: 0 for category in class_names}\n",
    "    summed_accuracy = 0\n",
    "\n",
    "    for report in rp_list:\n",
    "        for category, metrics in report.items():\n",
    "            if category == 'accuracy':\n",
    "                summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            if category in class_names_list and isinstance(metrics, dict):\n",
    "                if category not in summed_metrics:\n",
    "                    summed_metrics[category] = {key: 0 for key in metrics if key != 'support'}\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric != 'support':\n",
    "                        summed_metrics[category][metric] += value\n",
    "\n",
    "    for cm in cm_list:\n",
    "        for i, category in enumerate(class_names_list):\n",
    "            if category in summed_metrics:  # Ensure category exists in the metrics dictionary\n",
    "                TP = cm[i, i]\n",
    "                TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "                FP = cm[:, i].sum() - TP\n",
    "                FN = cm[i, :].sum() - TP\n",
    "                accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "                summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    # Calculate average metrics and accuracies\n",
    "    for category in class_names_list:\n",
    "        if category in summed_metrics:\n",
    "            avg_metrics[category] = {metric: summed_metrics[category][metric] / len(rp_list) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    \n",
    "    # Calculate macro and weighted averages\n",
    "    macro_avg = {metric: sum(avg_metrics[cat][metric] for cat in class_names_list) / len(class_names_list) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    weighted_avg = {metric: sum(avg_metrics[cat][metric] * summed_category_accuracies[cat] for cat in class_names_list) / sum(summed_category_accuracies.values()) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    \n",
    "    # Append macro and weighted averages to avg_metrics\n",
    "    avg_metrics['macro avg'] = macro_avg\n",
    "    avg_metrics['weighted avg'] = weighted_avg\n",
    "\n",
    "    avg_accuracy = summed_accuracy / len(rp_list) if len(rp_list) > 0 else 0\n",
    "    avg_category_accuracies = {category: summed_category_accuracies[category] / len(cm_list) for category in class_names_list}\n",
    "\n",
    "    return avg_metrics, avg_category_accuracies, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定相關參數並繪製圖表\n",
    "result_path = 'result/'  # 替換為你的結果路徑\n",
    "model_name = 'nn'         # 替換為你的模型名稱\n",
    "feature_type = 'bem'     # 替換為你的特徵類型\n",
    "scheme = 'address'            # 替換為你的方案\n",
    "class_names = ['Exchange', 'Faucet', 'Gambling', 'Market', 'Mixer', 'Pool']  # 替換為你的分類名稱\n",
    "n_folds = 5  # 替換為你的交叉驗證折數\n",
    "\n",
    "# 載入結果\n",
    "results, experiment_name = load_results(result_path, model_name, feature_type, scheme)\n",
    "\n",
    "# 顯示驗證集的平均混淆矩陣\n",
    "print('Average confusion matrix of validation set in {:d}-fold'.format(n_folds))\n",
    "show_cm_list(results['valid_cm_list'], class_names)\n",
    "\n",
    "# 顯示驗證集的平均分類報告和準確率\n",
    "print('Average classification report of validation set in {:d}-fold'.format(n_folds))\n",
    "avg_metrics, avg_category_accuracies, avg_accuracy = show_rp_list_and_accuracies(results['valid_rp_list'], results['valid_cm_list'], class_names)\n",
    "\n",
    "df_averages = pd.DataFrame(avg_metrics).T\n",
    "df_averages['accuracy'] = pd.Series(avg_category_accuracies)\n",
    "\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics for Validation Set')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義顯示分類報告和準確率的函數\n",
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    # Initialize dictionaries for summed and average metrics\n",
    "    summed_metrics, avg_metrics = {}, {}\n",
    "    summed_category_accuracies, avg_category_accuracies = {}, {}\n",
    "    summed_accuracy = 0\n",
    "\n",
    "    # Initialize summed metrics and accuracies\n",
    "    for category in class_names:\n",
    "        summed_metrics[category] = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "        summed_category_accuracies[category] = 0\n",
    "\n",
    "    # Iterate over reports and confusion matrices\n",
    "    for report_index, report in enumerate(rp_list):\n",
    "        for category, metrics in report.items():\n",
    "            # Skip 'accuracy', 'macro avg', and 'weighted avg' categories\n",
    "            if category in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                if category == 'accuracy':\n",
    "                    summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            # Sum metrics for each category\n",
    "            for metric in ['precision', 'recall', 'f1-score']:\n",
    "                if metric in metrics:\n",
    "                    summed_metrics[category][metric] += metrics[metric]\n",
    "\n",
    "        # Calculate and sum category-specific accuracies\n",
    "        cm = cm_list[report_index]\n",
    "        for i, category in enumerate(class_names):\n",
    "            TP = cm[i, i]\n",
    "            TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "            summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    # Calculate average metrics and accuracies\n",
    "    for category in class_names:\n",
    "        avg_metrics[category] = {metric: value / len(rp_list) for metric, value in summed_metrics[category].items()}\n",
    "        avg_category_accuracies[category] = summed_category_accuracies[category] / len(cm_list)\n",
    "    avg_accuracy = summed_accuracy / len(rp_list)\n",
    "\n",
    "    # Return average metrics and accuracies\n",
    "    return avg_metrics, avg_category_accuracies, avg_accuracy\n",
    "\n",
    "# 使用該函數來計算驗證集的平均分類報告和準確率\n",
    "avg_metrics, avg_category_accuracies, avg_accuracy = show_rp_list_and_accuracies(results['valid_rp_list'], results['valid_cm_list'], class_names)\n",
    "\n",
    "# 準備繪製的 DataFrame\n",
    "df_averages = pd.DataFrame(avg_metrics).T\n",
    "df_averages['accuracy'] = pd.Series(avg_category_accuracies)\n",
    "\n",
    "# 繪製條形圖\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics for Validation Set')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# 為每個條形圖添加數值標籤\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定義特徵\n",
    "# basic = [\n",
    "#     'f_tx', 'f_received', 'f_coinbase',\n",
    "#     'f_spent_digits_-3', 'f_spent_digits_-2', 'f_spent_digits_-1', 'f_spent_digits_0',\n",
    "#     'f_spent_digits_1', 'f_spent_digits_2', 'f_spent_digits_3', 'f_spent_digits_4',\n",
    "#     'f_spent_digits_5', 'f_spent_digits_6', 'f_received_digits_-3', 'f_received_digits_-2',\n",
    "#     'f_received_digits_-1', 'f_received_digits_0', 'f_received_digits_1', 'f_received_digits_2',\n",
    "#     'f_received_digits_3', 'f_received_digits_4', 'f_received_digits_5', 'f_received_digits_6',\n",
    "#     'r_payback', 'n_inputs_in_spent', 'n_outputs_in_spent'\n",
    "# ]\n",
    "# extra = [\n",
    "#     'n_tx', 'total_days', 'n_spent', 'n_received', 'n_coinbase', 'n_payback',\n",
    "#     'total_spent_btc', 'total_received_btc',\n",
    "#     'total_spent_usd', 'total_received_usd',\n",
    "#     'mean_balance_btc', 'std_balance_btc',\n",
    "#     'mean_balance_usd', 'std_balance_usd'\n",
    "# ]\n",
    "# moments = [\n",
    "#     'interval_1st_moment', 'interval_2nd_moment', 'interval_3rd_moment', 'interval_4th_moment',\n",
    "#     'dist_total_1st_moment', 'dist_total_2nd_moment', 'dist_total_3rd_moment', 'dist_total_4th_moment',\n",
    "#     'dist_coinbase_1st_moment', 'dist_coinbase_2nd_moment', 'dist_coinbase_3rd_moment', 'dist_coinbase_4th_moment',\n",
    "#     'dist_spend_1st_moment', 'dist_spend_2nd_moment', 'dist_spend_3rd_moment', 'dist_spend_4th_moment',\n",
    "#     'dist_receive_1st_moment', 'dist_receive_2nd_moment', 'dist_receive_3rd_moment', 'dist_receive_4th_moment',\n",
    "#     'dist_payback_1st_moment', 'dist_payback_2nd_moment', 'dist_payback_3rd_moment', 'dist_payback_4th_moment'\n",
    "# ]\n",
    "# patterns =[\n",
    "#     'tx_input', 'tx_output',\n",
    "#     'n_multi_in', 'n_multi_out', 'n_multi_in_out'\n",
    "# ]\n",
    "\n",
    "# features = []\n",
    "# if not feature_type.startswith('if') and len(feature_type) > 0:\n",
    "#     if 'b' in feature_type:\n",
    "#         features += basic\n",
    "#     if 'e' in feature_type:\n",
    "#         features += extra\n",
    "#     if 'm' in feature_type:\n",
    "#         features += moments\n",
    "#     if 'p' in feature_type:\n",
    "#         features += patterns\n",
    "# elif feature_type.startswith('if') and feature_type[2:].isdigit():\n",
    "#     all_features = basic + extra + moments + patterns\n",
    "#     if_indices = [\n",
    "#         0, 25, 24, 29, 40, 37, 27, 23, 56, 36,\n",
    "#         1, 28, 26, 57, 32, 38, 44, 45, 33, 18,\n",
    "#         39, 60, 53, 35, 34, 52, 41, 17, 14, 15,\n",
    "#         16, 19, 42, 5, 6, 47, 7, 46, 2, 54,\n",
    "#         4, 43, 8, 59, 58, 55, 9, 13, 61, 48,\n",
    "#         3, 31, 10, 62, 20, 21, 63, 30, 49, 11,\n",
    "#         51, 50, 22, 12\n",
    "#     ]\n",
    "#     if_features = [all_features[i] for i in if_indices]\n",
    "#     n_if = int(feature_type[2:])\n",
    "#     features = if_features[:n_if]\n",
    "# else:\n",
    "#     raise Exception('Invalid feature types: {:s}'.format(feature_type))\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 確保有特徵重要性數據\n",
    "# if len(results['fi_list']) == 0:\n",
    "#     raise ValueError(\"No feature importance data found.\")\n",
    "\n",
    "# # 計算平均特徵重要性\n",
    "# fi_avg = np.mean(results['fi_list'], axis=0)\n",
    "\n",
    "# # 打印特徵重要性的排序\n",
    "# print(fi_avg.argsort()[::-1])\n",
    "\n",
    "# # 準備特徵重要性數據的 DataFrame\n",
    "# df_feature_importances = pd.DataFrame({'name': features, 'importance': fi_avg})\n",
    "\n",
    "# # 選取前10個重要特徵\n",
    "# df_top_10 = df_feature_importances.nlargest(10, columns='importance')\n",
    "\n",
    "# # 繪製特徵重要性條形圖\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.barplot(x='importance', y='name', data=df_top_10)\n",
    "# plt.title('Top 10 Feature Importances')\n",
    "# plt.xlabel('Importance')\n",
    "# plt.ylabel('Feature Name')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# 計算驗證集的平均混淆矩陣\n",
    "cm_avg = np.mean(results['valid_cm_list'], axis=0)\n",
    "\n",
    "# 打印平均混淆矩陣\n",
    "print(cm_avg)\n",
    "\n",
    "# 確保在Jupyter Notebook中運行\n",
    "def run_from_ipython():\n",
    "    try:\n",
    "        __IPYTHON__\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "if run_from_ipython():\n",
    "    # 準備數據框\n",
    "    df_cm = pd.DataFrame(cm_avg, index=class_names, columns=class_names)\n",
    "    \n",
    "    # 設置圖表大小\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # 使用輕色調的配色方案\n",
    "    cmap = sns.light_palette('black', as_cmap=True)\n",
    "    \n",
    "    # 設置字體比例\n",
    "    sns.set(font_scale=1.6)\n",
    "    \n",
    "    # 繪製熱力圖\n",
    "    sns.heatmap(df_cm.round(2), annot=True, square=True, cbar=False, cmap=cmap)\n",
    "    \n",
    "    # 設置x和y軸標籤的旋轉角度\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 恢復字體比例\n",
    "    sns.set(font_scale=1)\n",
    "    \n",
    "    # 自動調整圖表佈局\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 顯示圖表\n",
    "    plt.show()\n",
    "    # 保存圖表到文件（如果需要保存，取消下面這行的註釋）\n",
    "    # plt.savefig('confusion_matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 確保在前面的程式碼中已經計算了以下變數\n",
    "# train_cm_list, valid_cm_list, train_auc_list, valid_auc_list\n",
    "\n",
    "# 計算平均混淆矩陣\n",
    "avg_train_cm = np.mean(results['train_cm_list'], axis=0)\n",
    "avg_valid_cm = np.mean(results['valid_cm_list'], axis=0)\n",
    "\n",
    "# 計算平均AUC分數\n",
    "avg_train_auc = np.mean(results['train_auc_list'])\n",
    "avg_valid_auc = np.mean(results['valid_auc_list'])\n",
    "\n",
    "# 展示平均混淆矩陣\n",
    "print(\"Average Training Confusion Matrix:\")\n",
    "print(avg_train_cm)\n",
    "print(\"\\nAverage Validation Confusion Matrix:\")\n",
    "print(avg_valid_cm)\n",
    "\n",
    "# 展示平均AUC分數\n",
    "print(f\"\\nAverage Train AUC: {avg_train_auc}\")\n",
    "print(f\"Average Valid AUC: {avg_valid_auc}\")\n",
    "\n",
    "# 繪製AUC分數圖表\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['train_auc_list'], label='Train AUC', marker='o')\n",
    "plt.plot([avg_train_auc] * len(results['train_auc_list']), 'r--', label='Average Train AUC')\n",
    "plt.plot(results['valid_auc_list'], label='Valid AUC', marker='o')\n",
    "plt.plot([avg_valid_auc] * len(results['valid_auc_list']), 'g--', label='Average Valid AUC')\n",
    "plt.title('AUC Scores per Fold')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假設 'class_names' 是一個已定義的列表，包含所有類別的名稱\n",
    "n_classes = len(class_names)  # 類別的數量\n",
    "\n",
    "# 使用儲存的 'y_valid' 和 'y_valid_prob'\n",
    "y_valid = np.array(results['y_valid'])\n",
    "y_valid_prob = np.array(results['y_valid_prob'])\n",
    "\n",
    "# 計算每個類別的FPR和TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# 計算每個類別的ROC曲線和AUC分數\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_valid, y_valid_prob[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# 計算宏觀平均ROC曲線和AUC分數\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# 繪製所有類別的ROC曲線\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(class_names[i], roc_auc[i]))\n",
    "\n",
    "# 繪製宏觀平均ROC曲線\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "# 繪製對角線\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC and AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
