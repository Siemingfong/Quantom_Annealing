{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2019 Willy Po-Wei Wu <maya6282@gmail.com> and Elvis Yu-Jing Lin <elvisyjlin@gmail.com>\n",
    "\n",
    "This work is licensed under the Creative Commons Attribution-NonCommercial\n",
    "4.0 International License. To view a copy of this license, visit\n",
    "http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
    "Creative Commons, PO Box 1866, Mountain View, CA 94042, USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Abbreviations\n",
    "1. cm: confusion matrix\n",
    "2. rp: classification report\n",
    "3. fi: feature importance\n",
    "4. if: important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all used packages\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from learn import get_model, get_params\n",
    "from utils import run_from_ipython, np2df\n",
    "from viz import show_cm_list, show_rp_list\n",
    "\n",
    "if run_from_ipython():\n",
    "    import matplotlib\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set_context('notebook')  # 'notebook', 'paper', 'talk', 'poster'\n",
    "    # sns.set_style('dark')  # None, 'darkgrid', 'whitegrid', 'dark', 'white', 'ticks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(feature_type='bem', gpu=False, n_folds=10, output='./data_p', result='./result', scheme='address', temp='./temp')\n"
     ]
    }
   ],
   "source": [
    "# Parse Arguments\n",
    "\n",
    "def parse(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog='Classification',\n",
    "        description='Train and test a machine learning classification method on the extracted features.'\n",
    "    )\n",
    "    parser.add_argument('--n_folds', help='n folds cross validation', type=int, default=10)\n",
    "    parser.add_argument('--feature_type', '-f',\n",
    "                        help='feature type (\"b\" | \"e\" | \"m\" or \"if4\", \"if5\", \"if10\", \"if13\", \"if20\", \"if64\")',\n",
    "                        type=str, default='bem')\n",
    "    parser.add_argument('--scheme', '-s', help='data scheme', type=str,\n",
    "                        choices=['address', 'entity'], default='address')\n",
    "    parser.add_argument('--gpu', help='use GPU', action='store_true')\n",
    "    parser.add_argument('--output', '-o', help='output path', type=str, default='./data_p')\n",
    "    parser.add_argument('--result', '-r', help='result path', type=str, default='./result')\n",
    "    parser.add_argument('--temp', '-t', help='temp path', type=str, default='./temp')\n",
    "    return parser.parse_args() if args is None else parser.parse_args(args)\n",
    "args = parse([]) if run_from_ipython() else parse()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Setting\n",
      "===> Feature Types:   bem\n",
      "===> Data Scheme:     address\n",
      "===> Use GPU:         False\n",
      "===> Training epochs: 4000\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment setting\n",
    "\n",
    "n_folds = args.n_folds                       # 10\n",
    "feature_type = args.feature_type             # 'b' | 'e' | 'm' or 'if4', 'if5', 'if10', 'if13', 'if20', 'if64'\n",
    "scheme = args.scheme                         # 'address', 'entity'\n",
    "gpu = args.gpu\n",
    "output_path = args.output\n",
    "result_path = args.result\n",
    "temp_path = args.temp\n",
    "\n",
    "# Check the experiment setting\n",
    "\n",
    "assert not feature_type.startswith('if') and len(feature_type) > 0 or \\\n",
    "       feature_type.startswith('if') and feature_type[2:].isdigit()\n",
    "assert scheme in ['address', 'entity']\n",
    "\n",
    "# Number of epochs to train\n",
    "# The specified epochs are enough to converge for each scheme\n",
    "if scheme == 'address':\n",
    "    epochs = 4000\n",
    "elif scheme == 'entity':\n",
    "    epochs = 1500\n",
    "\n",
    "# Show the experiment setting\n",
    "\n",
    "print('Experiment Setting')\n",
    "print('===> Feature Types:  ', feature_type)\n",
    "print('===> Data Scheme:    ', scheme)\n",
    "print('===> Use GPU:        ', gpu)\n",
    "print('===> Training epochs:', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             n_tx  total_days  total_spent_btc  total_received_btc  \\\n",
      "0        0.217092    0.320224        -0.016397           -0.016397   \n",
      "1        0.376189    0.179740         0.226984            0.226984   \n",
      "2       -0.180649   -0.400931        -0.021364           -0.021364   \n",
      "3       -0.180649   -0.400931        -0.021265           -0.021265   \n",
      "4       -0.180649   -0.400931        -0.021286           -0.021286   \n",
      "...           ...         ...              ...                 ...   \n",
      "1857029 -0.180649   -0.400931        -0.021296           -0.021296   \n",
      "1857030 -0.180649   -0.400931        -0.021296           -0.021296   \n",
      "1857031  0.057996   -0.354103         0.075168            0.075168   \n",
      "1857032 -0.101101   -0.400931        -0.015696           -0.015696   \n",
      "1857033 -0.180649   -0.400931        -0.019443           -0.019443   \n",
      "\n",
      "         total_spent_usd  total_received_usd  mean_balance_btc  \\\n",
      "0              -0.012077           -0.012077         -0.104385   \n",
      "1               0.100134            0.100134          0.938592   \n",
      "2              -0.014189           -0.014189         -0.132213   \n",
      "3              -0.014144           -0.014144         -0.128775   \n",
      "4              -0.014186           -0.014186         -0.129497   \n",
      "...                  ...                 ...               ...   \n",
      "1857029        -0.014184           -0.014184         -0.129840   \n",
      "1857030        -0.014184           -0.014184         -0.129840   \n",
      "1857031        -0.007944           -0.007944          0.700159   \n",
      "1857032        -0.013939           -0.013939         -0.034832   \n",
      "1857033        -0.013767           -0.013767         -0.065902   \n",
      "\n",
      "         std_balance_btc  mean_balance_usd  std_balance_usd  ...  \\\n",
      "0              -0.002449         -0.053183        -0.002642  ...   \n",
      "1               0.001403          0.319334        -0.002180  ...   \n",
      "2              -0.002451         -0.062551        -0.002643  ...   \n",
      "3              -0.002451         -0.061351        -0.002643  ...   \n",
      "4              -0.002451         -0.062477        -0.002643  ...   \n",
      "...                  ...               ...              ...  ...   \n",
      "1857029        -0.002451         -0.062418        -0.002643  ...   \n",
      "1857030        -0.002451         -0.062427        -0.002643  ...   \n",
      "1857031         0.000025         -0.020863        -0.002635  ...   \n",
      "1857032        -0.002451         -0.059229        -0.002643  ...   \n",
      "1857033        -0.002451         -0.051277        -0.002643  ...   \n",
      "\n",
      "         dist_payback_1st_moment  dist_payback_2nd_moment  \\\n",
      "0                       0.214867                -0.114233   \n",
      "1                       0.327479                -0.134258   \n",
      "2                      -0.389038                -0.173049   \n",
      "3                      -0.389038                -0.173049   \n",
      "4                      -0.389038                -0.173049   \n",
      "...                          ...                      ...   \n",
      "1857029                -0.389038                -0.173049   \n",
      "1857030                -0.389038                -0.173049   \n",
      "1857031                -0.341500                -0.172533   \n",
      "1857032                -0.388843                -0.173049   \n",
      "1857033                -0.389038                -0.173049   \n",
      "\n",
      "         dist_payback_3rd_moment  dist_payback_4th_moment   tx_input  \\\n",
      "0                       1.410258                 1.072335  -0.151659   \n",
      "1                      -0.992143                 0.774267   0.186267   \n",
      "2                      -0.132643                -0.518267  -0.151659   \n",
      "3                      -0.132643                -0.518267  -0.151659   \n",
      "4                      -0.132643                -0.518267   0.298909   \n",
      "...                          ...                      ...        ...   \n",
      "1857029                -0.132643                -0.518267  -0.039017   \n",
      "1857030                -0.132643                -0.518267  23.277854   \n",
      "1857031                 0.404323                 0.367838  -0.039017   \n",
      "1857032                -0.132643                 0.139411  -0.151659   \n",
      "1857033                -0.132643                -0.518267  -0.039017   \n",
      "\n",
      "         tx_output  n_multi_in  n_multi_out  n_multi_in_out  class  \n",
      "0        -0.055827   -0.219166    -0.050256       -0.657671      0  \n",
      "1        -0.055827    0.181707    -0.050256        1.520518      0  \n",
      "2        -0.055827   -0.219166    -0.050256       -0.657671      1  \n",
      "3        -0.055827   -0.219166    -0.050256       -0.657671      0  \n",
      "4        -0.055827    0.281925    -0.050256        1.520518      0  \n",
      "...            ...         ...          ...             ...    ...  \n",
      "1857029  -0.055827   -0.018729    -0.050256        1.520518      4  \n",
      "1857030  -0.055827   20.726437    -0.050256        1.520518      5  \n",
      "1857031  -0.055827   -0.018729    -0.050256        1.520518      5  \n",
      "1857032  -0.055827   -0.219166    -0.050256       -0.657671      2  \n",
      "1857033  -0.138124   -0.018729    -0.206951       -0.657671      2  \n",
      "\n",
      "[1857034 rows x 74 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load transaction history summarization data\n",
    "\n",
    "# data_file = 'data.{}.csv'.format(scheme)\n",
    "data_file = 'nanzero_normalization_data.{}.csv'.format(scheme)\n",
    "data = pd.read_csv(os.path.join(output_path, data_file))\n",
    "print (data)\n",
    "if run_from_ipython():\n",
    "    data.head(5)\n",
    "else:\n",
    "    print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f_tx', 'f_received', 'f_coinbase', 'f_spent_digits_-3', 'f_spent_digits_-2', 'f_spent_digits_-1', 'f_spent_digits_0', 'f_spent_digits_1', 'f_spent_digits_2', 'f_spent_digits_3', 'f_spent_digits_4', 'f_spent_digits_5', 'f_spent_digits_6', 'f_received_digits_-3', 'f_received_digits_-2', 'f_received_digits_-1', 'f_received_digits_0', 'f_received_digits_1', 'f_received_digits_2', 'f_received_digits_3', 'f_received_digits_4', 'f_received_digits_5', 'f_received_digits_6', 'r_payback', 'n_inputs_in_spent', 'n_outputs_in_spent', 'n_tx', 'total_days', 'n_spent', 'n_received', 'n_coinbase', 'n_payback', 'total_spent_btc', 'total_received_btc', 'total_spent_usd', 'total_received_usd', 'mean_balance_btc', 'std_balance_btc', 'mean_balance_usd', 'std_balance_usd', 'interval_1st_moment', 'interval_2nd_moment', 'interval_3rd_moment', 'interval_4th_moment', 'dist_total_1st_moment', 'dist_total_2nd_moment', 'dist_total_3rd_moment', 'dist_total_4th_moment', 'dist_coinbase_1st_moment', 'dist_coinbase_2nd_moment', 'dist_coinbase_3rd_moment', 'dist_coinbase_4th_moment', 'dist_spend_1st_moment', 'dist_spend_2nd_moment', 'dist_spend_3rd_moment', 'dist_spend_4th_moment', 'dist_receive_1st_moment', 'dist_receive_2nd_moment', 'dist_receive_3rd_moment', 'dist_receive_4th_moment', 'dist_payback_1st_moment', 'dist_payback_2nd_moment', 'dist_payback_3rd_moment', 'dist_payback_4th_moment']\n",
      "bem\n",
      "[[-0.46117286  0.0030658  -0.0030658  ... -0.11423308  1.4102578\n",
      "   1.07233516]\n",
      " [-0.43005299  0.0030658  -0.0030658  ... -0.13425807 -0.99214269\n",
      "   0.77426704]\n",
      " [ 0.11264715  0.0030658  -0.0030658  ... -0.17304888 -0.13264268\n",
      "  -0.51826731]\n",
      " ...\n",
      " [-0.09456563  0.0030658  -0.0030658  ... -0.17253313  0.4043231\n",
      "   0.36783783]\n",
      " [ 0.7342855   0.0030658  -0.0030658  ... -0.17304887 -0.13264268\n",
      "   0.13941113]\n",
      " [ 0.11264715  0.0030658  -0.0030658  ... -0.17304888 -0.13264268\n",
      "  -0.51826731]]\n",
      "[0 0 1 ... 5 2 2]\n",
      "['Exchange' 'Faucet' 'Gambling' 'Market' 'Mixer' 'Pool']\n",
      "1857034 1857034 64\n"
     ]
    }
   ],
   "source": [
    "# Define 4 types of features (basic statistics, extra statistics, moments and patterns)\n",
    "\n",
    "basic = [\n",
    "    'f_tx', 'f_received', 'f_coinbase',\n",
    "    'f_spent_digits_-3', 'f_spent_digits_-2', 'f_spent_digits_-1', 'f_spent_digits_0',\n",
    "    'f_spent_digits_1', 'f_spent_digits_2', 'f_spent_digits_3', 'f_spent_digits_4',\n",
    "    'f_spent_digits_5', 'f_spent_digits_6', 'f_received_digits_-3', 'f_received_digits_-2',\n",
    "    'f_received_digits_-1', 'f_received_digits_0', 'f_received_digits_1', 'f_received_digits_2',\n",
    "    'f_received_digits_3', 'f_received_digits_4', 'f_received_digits_5', 'f_received_digits_6',\n",
    "    'r_payback', 'n_inputs_in_spent', 'n_outputs_in_spent'\n",
    "]\n",
    "extra = [\n",
    "    'n_tx', 'total_days', 'n_spent', 'n_received', 'n_coinbase', 'n_payback',\n",
    "    'total_spent_btc', 'total_received_btc',\n",
    "    'total_spent_usd', 'total_received_usd',\n",
    "    'mean_balance_btc', 'std_balance_btc',\n",
    "    'mean_balance_usd', 'std_balance_usd'\n",
    "]\n",
    "moments = [\n",
    "    'interval_1st_moment', 'interval_2nd_moment', 'interval_3rd_moment', 'interval_4th_moment',\n",
    "    'dist_total_1st_moment', 'dist_total_2nd_moment', 'dist_total_3rd_moment', 'dist_total_4th_moment',\n",
    "    'dist_coinbase_1st_moment', 'dist_coinbase_2nd_moment', 'dist_coinbase_3rd_moment', 'dist_coinbase_4th_moment',\n",
    "    'dist_spend_1st_moment', 'dist_spend_2nd_moment', 'dist_spend_3rd_moment', 'dist_spend_4th_moment',\n",
    "    'dist_receive_1st_moment', 'dist_receive_2nd_moment', 'dist_receive_3rd_moment', 'dist_receive_4th_moment',\n",
    "    'dist_payback_1st_moment', 'dist_payback_2nd_moment', 'dist_payback_3rd_moment', 'dist_payback_4th_moment'\n",
    "]\n",
    "patterns =[\n",
    "    'tx_input', 'tx_output',\n",
    "    'n_multi_in', 'n_multi_out', 'n_multi_in_out'\n",
    "]\n",
    "\n",
    "features = []\n",
    "if not feature_type.startswith('if') and len(feature_type) > 0:\n",
    "    if 'b' in feature_type:\n",
    "        features += basic\n",
    "    if 'e' in feature_type:\n",
    "        features += extra\n",
    "    if 'm' in feature_type:\n",
    "        features += moments\n",
    "    if 'p' in feature_type:\n",
    "        features += patterns\n",
    "        print(\"Patterns included:\", patterns)\n",
    "elif feature_type.startswith('if') and feature_type[2:].isdigit():\n",
    "    \"\"\"\n",
    "    Important features from LightGBM with BEM\n",
    "    [ 0 25 24 29 40 37 27 23 56 36  1 28 26 57 32 38 44 45 33 18 39 60 53 35\n",
    "     34 52 41 17 14 15 16 19 42  5  6 47  7 46  2 54  4 43  8 59 58 55  9 13\n",
    "     61 48  3 31 10 62 20 21 63 30 49 11 51 50 22 12]\n",
    "    \"\"\"\n",
    "    all_features = basic + extra + moments + patterns\n",
    "    if_indices = [\n",
    "        0, 25, 24, 29, 40, 37, 27, 23, 56, 36,\n",
    "        1, 28, 26, 57, 32, 38, 44, 45, 33, 18,\n",
    "        39, 60, 53, 35, 34, 52, 41, 17, 14, 15,\n",
    "        16, 19, 42, 5, 6, 47, 7, 46, 2, 54,\n",
    "        4, 43, 8, 59, 58, 55, 9, 13, 61, 48,\n",
    "        3, 31, 10, 62, 20, 21, 63, 30, 49, 11,\n",
    "        51, 50, 22, 12\n",
    "    ]\n",
    "    if_features = [all_features[i] for i in if_indices]\n",
    "    n_if = int(feature_type[2:])\n",
    "    features = if_features[:n_if]\n",
    "else:\n",
    "    raise Exception('Invalid feature types: {:s}'.format(feature_type))\n",
    "\n",
    "invalid_features = [feature for feature in features if feature not in data.columns]\n",
    "assert len(invalid_features) == 0, 'Invalid features: ' + ', '.join(invalid_features)\n",
    "\n",
    "X = data.get(features).values\n",
    "y = data['class'].values\n",
    "print (features)\n",
    "print (feature_type)\n",
    "print (X)\n",
    "print (y)\n",
    "\n",
    "class2label = json.loads(open(os.path.join(output_path, 'class2label.json'), 'r').read())\n",
    "label2class = json.loads(open(os.path.join(output_path, 'label2class.json'), 'r').read())\n",
    "class_names = np.array([label2class[i] for i in range(6)])\n",
    "print (class_names)\n",
    "y_names = class_names\n",
    "# y_names = class_names[y]\n",
    "# y_names = np.array(class_names)[y.astype(int)]\n",
    "\n",
    "print(len(X), len(y), len(features))\n",
    "\n",
    "os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_distribution(df):\n",
    "#     if run_from_ipython():\n",
    "#         plt.figure()\n",
    "#         sns.countplot(df.index)\n",
    "#     cnt = collections.Counter(df.index)\n",
    "#     print(cnt)\n",
    "#     return np.array([cnt[i] for i in range(len(cnt))])\n",
    "\n",
    "# print(class2label)\n",
    "# y_count = data_distribution(np2df(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "# 如果不想使用 GPU，可以設置環境變量\n",
    "if not gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# GPU 配置 for TensorFlow 2.x\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 為每個 GPU 設置允許記憶體增長\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def build_model(input_size, num_classes, summary=False):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_size,)),\n",
    "        BatchNormalization(momentum=0.0, epsilon=1e-5),\n",
    "        Dropout(0.2),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(momentum=0.0, epsilon=1e-5),\n",
    "        Dropout(0.2),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(momentum=0.0, epsilon=1e-5),\n",
    "        Dropout(0.2),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(momentum=0.0, epsilon=1e-5),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               33280     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 512)               2048      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 832518 (3.18 MB)\n",
      "Trainable params: 828422 (3.16 MB)\n",
      "Non-trainable params: 4096 (16.00 KB)\n",
      "_________________________________________________________________\n",
      "0.26750096678733826 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIEMINGFONG\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4114612638950348 1\n",
      "0.46399107575416565 2\n",
      "0.49566513299942017 3\n",
      "0.5098490118980408 6\n",
      "0.5111790895462036 13\n",
      "0.5269784331321716 15\n",
      "0.5276892185211182 20\n",
      "0.5358796715736389 21\n",
      "0.5415068864822388 22\n",
      "0.542939305305481 23\n",
      "0.5648505091667175 24\n",
      "0.5707846879959106 25\n",
      "0.6829416751861572 26\n",
      "0.6930491328239441 29\n",
      "0.7063176035881042 31\n",
      "0.7120363712310791 34\n",
      "0.7123540639877319 37\n",
      "0.7161288857460022 39\n",
      "0.7168935537338257 43\n",
      "0.7213307023048401 44\n",
      "0.7227738499641418 49\n",
      "0.7234631180763245 51\n",
      "0.7272648811340332 58\n",
      "0.729849636554718 61\n",
      "0.7301889061927795 66\n",
      "0.7317397594451904 70\n",
      "0.7321867346763611 71\n",
      "0.7343621850013733 75\n",
      "0.737113893032074 78\n",
      "0.7391332387924194 82\n",
      "0.7411795258522034 83\n",
      "0.7440658211708069 88\n",
      "0.7458428740501404 95\n",
      "0.74751216173172 102\n",
      "0.7483845353126526 112\n",
      "0.749574601650238 115\n",
      "0.7501077055931091 119\n",
      "0.7525901198387146 131\n",
      "0.7526655197143555 133\n",
      "0.7545987367630005 143\n",
      "0.7550725936889648 151\n",
      "0.7556218504905701 153\n",
      "0.756413459777832 154\n",
      "0.7569357752799988 162\n",
      "0.7580612301826477 165\n",
      "0.7585781812667847 170\n",
      "0.758868932723999 179\n",
      "0.7590412497520447 182\n",
      "0.760064423084259 184\n",
      "0.7603175044059753 185\n"
     ]
    }
   ],
   "source": [
    "import time  # 加入時間模組\n",
    "\n",
    "# 訓練過程\n",
    "train_cm_list = []\n",
    "train_rp_list = []\n",
    "valid_cm_list = []\n",
    "valid_rp_list = []\n",
    "fi_list = []\n",
    "\n",
    "# Add these lists to store AUC scores\n",
    "train_auc_list = []\n",
    "valid_auc_list = []\n",
    "\n",
    "# 宣告 K-Fold\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "# # 標準化資料\n",
    "# print('Normalizing data...')\n",
    "# X = np.nan_to_num((X - np.mean(X, axis=0)) / np.std(X, axis=0))\n",
    "# X = np.clip(X, np.percentile(X, 1, axis=0), np.percentile(X, 99, axis=0))\n",
    "\n",
    "# 開始計算整體訓練時間\n",
    "total_start_time = time.time()  # 全部訓練開始時間\n",
    "\n",
    "# 開始交叉驗證\n",
    "for i, (train_idx, valid_idx) in tqdm(enumerate(skf.split(X, y))):\n",
    "    # 取得分割的訓練集和驗證集\n",
    "    X_train, X_valid = X[train_idx], X[valid_idx]\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "    y_train_ = keras.utils.to_categorical(y_train, num_classes=len(class2label))\n",
    "    y_valid_ = keras.utils.to_categorical(y_valid, num_classes=len(class2label))\n",
    "    \n",
    "    # 建立模型\n",
    "    model = build_model(X.shape[1], len(label2class), summary=(i==0))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.999),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # 訓練模型\n",
    "    acc = -1\n",
    "    for epoch in range(epochs):\n",
    "        model.train_on_batch(X_train, y_train_)\n",
    "        accs = model.test_on_batch(X_valid, y_valid_)\n",
    "        if accs[1] > acc:\n",
    "            acc = accs[1]\n",
    "            print(acc, epoch)\n",
    "            model.save(os.path.join(temp_path, 'model.h5'))\n",
    "    model = load_model(os.path.join(temp_path, 'model.h5'))\n",
    "    \n",
    "    # 在訓練集上評估\n",
    "    y_pred = np.argmax(model.predict(X_train), axis=1)\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    cm_sum = cm.sum(axis=1, keepdims=True)\n",
    "    cm_sum[cm_sum == 0] = 1  # 防止除零\n",
    "    cm = cm / cm_sum\n",
    "    train_cm_list.append(cm)\n",
    "    \n",
    "    # 在驗證集上評估\n",
    "    y_pred = np.argmax(model.predict(X_valid), axis=1)\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    cm = cm / cm.sum(axis=1, keepdims=True)\n",
    "    valid_cm_list.append(cm)\n",
    "    rp = classification_report(y_valid, y_pred, target_names=class_names, output_dict=True)\n",
    "    valid_rp_list.append(rp)\n",
    "    \n",
    "\n",
    "# 全部訓練結束時間\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time  # 計算總訓練時間\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")  # 輸出總訓練時間\n",
    "\n",
    "# 清除臨時檔案\n",
    "shutil.rmtree(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the result path if it does not exist\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "# Save training results\n",
    "\n",
    "experiment_name = os.path.join(result_path, 'nn.{}.{}'.format(feature_type, scheme))\n",
    "results = {\n",
    "    'train_cm_list': train_cm_list,\n",
    "    'valid_cm_list': valid_cm_list,\n",
    "    'train_rp_list': train_rp_list,\n",
    "    'valid_rp_list': valid_rp_list,\n",
    "    'fi_list': fi_list\n",
    "}\n",
    "pickle.dump(results, open(experiment_name + '.pkl', 'wb'))\n",
    "\n",
    "# Save model\n",
    "model_save_path = '{}_model.pkl'.format(experiment_name)\n",
    "with open(model_save_path, 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "print(f\"Results and model saved to {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average confusion matrix of training set in K-fold\n",
    "\n",
    "print('Average confusion matrix of training set in {:d}-fold'.format(n_folds))\n",
    "show_cm_list(train_cm_list, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average classification report of training set in K-fold\n",
    "\n",
    "print('Average classification report of training set in {:d}-fold'.format(n_folds))\n",
    "show_rp_list(train_rp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    summed_metrics = {}\n",
    "    summed_accuracy = 0\n",
    "    summed_category_accuracies = {category: 0 for category in class_names}\n",
    "    category_supports = {category: 0 for category in class_names}\n",
    "\n",
    "    for report_index, report in enumerate(rp_list):\n",
    "        for category, metrics in report.items():\n",
    "            if category == 'accuracy':\n",
    "                summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            if isinstance(metrics, dict) and category in class_names:\n",
    "                if category not in summed_metrics:\n",
    "                    summed_metrics[category] = {key: 0 for key in metrics if key != 'support'}\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric != 'support':\n",
    "                        summed_metrics[category][metric] += value\n",
    "                category_supports[category] += metrics.get('support', 0)\n",
    "\n",
    "        cm = cm_list[report_index]\n",
    "        for i, category in enumerate(class_names):\n",
    "            TP = cm[i, i]\n",
    "            TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "            summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    avg_metrics = {}\n",
    "    total_support = sum(category_supports.values())\n",
    "    macro_avg = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "    weighted_avg = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "\n",
    "    for category, metrics in summed_metrics.items():\n",
    "        avg_metrics[category] = {metric: value / len(rp_list) for metric, value in metrics.items()}\n",
    "        for metric in macro_avg:\n",
    "            macro_avg[metric] += avg_metrics[category][metric] / len(class_names)\n",
    "            weighted_avg[metric] += (avg_metrics[category][metric] * category_supports[category]) / total_support\n",
    "\n",
    "    avg_accuracy = summed_accuracy / len(rp_list)\n",
    "    avg_category_accuracies = {category: acc / len(cm_list) for category, acc in summed_category_accuracies.items()}\n",
    "\n",
    "    print(f\"Overall Accuracy: {avg_accuracy}\\n\")\n",
    "    for category, metrics in avg_metrics.items():\n",
    "        print(f\"Category: {category}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "        print(f\"  Accuracy: {avg_category_accuracies[category]}\\n\")\n",
    "\n",
    "    print(\"Category: macro avg\")\n",
    "    for metric, value in macro_avg.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\nCategory: weighted avg\")\n",
    "    for metric, value in weighted_avg.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Assuming n_folds, train_rp_list, train_cm_list, and class_names are defined elsewhere\n",
    "print('Average classification report and accuracies of training set in {:d}-fold'.format(n_folds))\n",
    "show_rp_list_and_accuracies(train_rp_list, train_cm_list, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average confusion matrix of validataion set in K-fold\n",
    "\n",
    "print('Average confusion matrix of validataion set in {:d}-fold'.format(n_folds))\n",
    "show_cm_list(valid_cm_list, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    class_names_list = list(class_names)  # Convert class_names to a list for index access\n",
    "    summed_metrics, avg_metrics = {}, {}\n",
    "    summed_category_accuracies = {category: 0 for category in class_names}\n",
    "    summed_accuracy = 0\n",
    "\n",
    "    for report in rp_list:\n",
    "        for category, metrics in report.items():\n",
    "            if category == 'accuracy':\n",
    "                summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            if category in class_names_list and isinstance(metrics, dict):\n",
    "                if category not in summed_metrics:\n",
    "                    summed_metrics[category] = {key: 0 for key in metrics if key != 'support'}\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric != 'support':\n",
    "                        summed_metrics[category][metric] += value\n",
    "\n",
    "    for cm in cm_list:\n",
    "        for i, category in enumerate(class_names_list):\n",
    "            if category in summed_metrics:  # Ensure category exists in the metrics dictionary\n",
    "                TP = cm[i, i]\n",
    "                TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "                FP = cm[:, i].sum() - TP\n",
    "                FN = cm[i, :].sum() - TP\n",
    "                accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "                summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    # Calculate average metrics and accuracies\n",
    "    for category in class_names_list:\n",
    "        if category in summed_metrics:\n",
    "            avg_metrics[category] = {metric: summed_metrics[category][metric] / len(rp_list) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    \n",
    "    # Calculate macro and weighted averages\n",
    "    macro_avg = {metric: sum(avg_metrics[cat][metric] for cat in class_names_list) / len(class_names_list) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    weighted_avg = {metric: sum(avg_metrics[cat][metric] * summed_category_accuracies[cat] for cat in class_names_list) / sum(summed_category_accuracies.values()) for metric in ['precision', 'recall', 'f1-score']}\n",
    "    \n",
    "    # Append macro and weighted averages to avg_metrics\n",
    "    avg_metrics['macro avg'] = macro_avg\n",
    "    avg_metrics['weighted avg'] = weighted_avg\n",
    "\n",
    "    avg_accuracy = summed_accuracy / len(rp_list) if len(rp_list) > 0 else 0\n",
    "    avg_category_accuracies = {category: summed_category_accuracies[category] / len(cm_list) for category in class_names_list}\n",
    "\n",
    "    return avg_metrics, avg_category_accuracies, avg_accuracy\n",
    "\n",
    "# Assuming necessary variables (n_folds, valid_rp_list, valid_cm_list, class_names) are defined\n",
    "print('Average classification report of validation set in {:d}-fold'.format(n_folds))\n",
    "avg_metrics, avg_category_accuracies, avg_accuracy = show_rp_list_and_accuracies(valid_rp_list, valid_cm_list, class_names)\n",
    "\n",
    "df_averages = pd.DataFrame(avg_metrics).T\n",
    "df_averages['accuracy'] = pd.Series(avg_category_accuracies)\n",
    "\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics for Validation Set')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rp_list_and_accuracies(rp_list, cm_list, class_names):\n",
    "    # Initialize dictionaries for summed and average metrics\n",
    "    summed_metrics, avg_metrics = {}, {}\n",
    "    summed_category_accuracies, avg_category_accuracies = {}, {}\n",
    "    summed_accuracy = 0\n",
    "\n",
    "    # Initialize summed metrics and accuracies\n",
    "    for category in class_names:\n",
    "        summed_metrics[category] = {'precision': 0, 'recall': 0, 'f1-score': 0}\n",
    "        summed_category_accuracies[category] = 0\n",
    "\n",
    "    # Iterate over reports and confusion matrices\n",
    "    for report_index, report in enumerate(rp_list):\n",
    "        for category, metrics in report.items():\n",
    "            # Skip 'accuracy', 'macro avg', and 'weighted avg' categories\n",
    "            if category in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                if category == 'accuracy':\n",
    "                    summed_accuracy += metrics\n",
    "                continue\n",
    "\n",
    "            # Sum metrics for each category\n",
    "            for metric in ['precision', 'recall', 'f1-score']:\n",
    "                if metric in metrics:\n",
    "                    summed_metrics[category][metric] += metrics[metric]\n",
    "\n",
    "        # Calculate and sum category-specific accuracies\n",
    "        cm = cm_list[report_index]\n",
    "        for i, category in enumerate(class_names):\n",
    "            TP = cm[i, i]\n",
    "            TN = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - TP)\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            accuracy = (TP + TN) / float(TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "            summed_category_accuracies[category] += accuracy\n",
    "\n",
    "    # Calculate average metrics and accuracies\n",
    "    for category in class_names:\n",
    "        avg_metrics[category] = {metric: value / len(rp_list) for metric, value in summed_metrics[category].items()}\n",
    "        avg_category_accuracies[category] = summed_category_accuracies[category] / len(cm_list)\n",
    "    avg_accuracy = summed_accuracy / len(rp_list)\n",
    "\n",
    "    # Return average metrics and accuracies\n",
    "    return avg_metrics, avg_category_accuracies, avg_accuracy\n",
    "\n",
    "# Use the function for validation set and get the data for plotting\n",
    "avg_metrics, avg_category_accuracies, avg_accuracy = show_rp_list_and_accuracies(valid_rp_list, valid_cm_list, class_names)\n",
    "\n",
    "# Prepare DataFrame for plotting\n",
    "df_averages = pd.DataFrame(avg_metrics).T\n",
    "df_averages['accuracy'] = pd.Series(avg_category_accuracies)\n",
    "\n",
    "# Plotting\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics for Validation Set')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add value labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "if len(fi_list) == 0:\n",
    "    exit()\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    print(clf.importance_type)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "fi_avg = np.mean(fi_list, axis=0)\n",
    "# print(fi_avg)\n",
    "print(fi_avg.argsort()[::-1])\n",
    "if run_from_ipython():\n",
    "    df_feature_importances = pd.DataFrame({'name': features, 'importance': fi_avg})\n",
    "    df_top_10 = df_feature_importances.nlargest(10, columns='importance')\n",
    "    plt.figure()\n",
    "    sns.barplot(x='importance', y='name', data=df_top_10)  # Modified this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_from_ipython():\n",
    "    plt.figure(figsize=(16, 32))\n",
    "    for i in range(10):\n",
    "        feature_index = df_top_10.index[i]\n",
    "        feature_name = list(df_top_10['name'])[i]\n",
    "        feature_data = X[:, feature_index]\n",
    "\n",
    "        # Check if the lengths match\n",
    "        if len(y_names) != len(feature_data):\n",
    "            print(f\"Length mismatch for feature '{feature_name}': Length of y_names is {len(y_names)}, length of feature data is {len(feature_data)}\")\n",
    "            continue  # Skip this iteration\n",
    "\n",
    "        plt.subplot(5, 2, i+1)\n",
    "        plt.title(feature_name)\n",
    "        ax = sns.boxplot(x=y_names, y=feature_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average classification report of validataion set in K-fold\n",
    "\n",
    "print('Average classification report of validataion set in {:d}-fold'.format(n_folds))\n",
    "show_rp_list(valid_rp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average classification report of validataion set in K-fold\n",
    "\n",
    "print('Average classification report of validataion set in {:d}-fold'.format(n_folds))\n",
    "show_rp_list(valid_rp_list)\n",
    "\n",
    "# 初始化用於存儲每個類別平均精確度、召回率和F1分數的字典\n",
    "averages = {}\n",
    "\n",
    "# 遍歷每個報告\n",
    "for report in valid_rp_list:\n",
    "    # 遍歷報告中的每個類別\n",
    "    for category, metrics in report.items():\n",
    "        # 確保 metrics 是字典類型\n",
    "        if isinstance(metrics, dict):\n",
    "            # 如果類別第一次出現，則初始化\n",
    "            if category not in averages:\n",
    "                averages[category] = {'precision': 0, 'recall': 0, 'f1-score': 0, 'count': 0}\n",
    "            \n",
    "            # 累加該類別的指標值\n",
    "            for metric in ['precision', 'recall', 'f1-score']:\n",
    "                averages[category][metric] += metrics.get(metric, 0)\n",
    "            averages[category]['count'] += 1\n",
    "\n",
    "# 計算平均值\n",
    "for category, metrics in averages.items():\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        metrics[metric] /= metrics['count']\n",
    "\n",
    "# 刪除計數鍵\n",
    "for category in averages:\n",
    "    del averages[category]['count']\n",
    "\n",
    "# 轉換成DataFrame\n",
    "df_averages = pd.DataFrame(averages).T\n",
    "\n",
    "# 繪製柱狀圖並添加數值標籤\n",
    "ax = df_averages.plot(kind='bar', figsize=(12, 8), width=0.8, alpha=0.75)\n",
    "ax.set_title('Average Classification Report Metrics')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_xlabel('Category')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# 在每個柱狀圖上添加數值標籤\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_avg = np.mean(fi_list, axis=0)\n",
    "# print(fi_avg)\n",
    "print(fi_avg.argsort()[::-1])\n",
    "if run_from_ipython():\n",
    "    df_feature_importances = pd.DataFrame({'name': features, 'importance': fi_avg})\n",
    "    df_least_10 = df_feature_importances.nsmallest(10, columns='importance')\n",
    "    plt.figure()\n",
    "    sns.barplot(x='importance', y='name', data=df_least_10)  # Corrected this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_avg = np.mean(fi_list, axis=0)\n",
    "print(fi_avg.argsort()[::-1])\n",
    "if run_from_ipython():\n",
    "    df_feature_importances = pd.DataFrame({'Feature Name': features, 'Importance': fi_avg})\n",
    "    df_least_10 = df_feature_importances.nlargest(len(features), columns='Importance')\n",
    "    plt.figure(figsize=(24, 48))\n",
    "    sns.set(font_scale=2)\n",
    "    sns.barplot(x='Importance', y='Feature Name', data=df_least_10)  # Corrected this line\n",
    "    sns.set(font_scale=1)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('top_10_and_last_10_feature_importance.png')  # Uncomment this line to save the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cm_avg = np.mean(valid_cm_list, axis=0)\n",
    "# print(cm_avg)\n",
    "if run_from_ipython():\n",
    "    df_cm = pd.DataFrame(cm_avg, index=class_names, columns=class_names)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    cmap = sns.light_palette('black', as_cmap=True)\n",
    "    # cmap = ListedColormap(['white'])\n",
    "    sns.set(font_scale=1.6)\n",
    "    sns.heatmap(df_cm.round(2), annot=True, square=True, cbar=False, cmap=cmap)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    sns.set(font_scale=1)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('confusion_matrix.png')  # Uncomment this line to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算平均混淆矩阵\n",
    "avg_train_cm = np.mean(train_cm_list, axis=0)\n",
    "avg_valid_cm = np.mean(valid_cm_list, axis=0)\n",
    "\n",
    "# 计算平均AUC分数\n",
    "avg_train_auc = np.mean(train_auc_list)\n",
    "avg_valid_auc = np.mean(valid_auc_list)\n",
    "\n",
    "# 展示平均混淆矩阵\n",
    "print(\"Average Training Confusion Matrix:\")\n",
    "print(avg_train_cm)\n",
    "print(\"\\nAverage Validation Confusion Matrix:\")\n",
    "print(avg_valid_cm)\n",
    "\n",
    "# 展示平均AUC分数\n",
    "print(f\"\\nAverage Train AUC: {avg_train_auc}\")\n",
    "print(f\"Average Valid AUC: {avg_valid_auc}\")\n",
    "\n",
    "# 绘制AUC分数图表\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_auc_list, label='Train AUC', marker='o')\n",
    "plt.plot([avg_train_auc] * len(train_auc_list), 'r--', label='Average Train AUC')\n",
    "plt.plot(valid_auc_list, label='Valid AUC', marker='o')\n",
    "plt.plot([avg_valid_auc] * len(valid_auc_list), 'g--', label='Average Valid AUC')\n",
    "plt.title('AUC Scores per Fold')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# 计算每个类别的FPR和TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = len(class_names)  # 类别的数量\n",
    "\n",
    "# 计算每个类别的ROC曲线和AUC分数\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_valid, y_valid_prob[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# 计算宏观平均ROC曲线和AUC分数\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# 绘制所有类别的ROC曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "# 绘制宏观平均ROC曲线\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "# 绘制对角线\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC and AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
